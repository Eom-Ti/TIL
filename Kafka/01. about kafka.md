# About Kafka
## Kafka
수십년 동안 우리는 데이터베이스를 사용해왔음. 이는 데이터베이스에 정보를 저장하는 프로그램을 의미하며, 세상을 사물의 관점에서 생각하도록 하는 바잇ㄱ이 주를 이룸.
이러한 사물들의 상태를 가져와 데이터베이스에 저장하는 방식은 효율적이었으나, 이런 생각들이 사물 -> 사건을 먼저생각하는 것이 더 낫다는 것을 알게되고 있음.
이러한 사건은 시간성이 중요하며, 이는 데이터베이스에서 관리하기가 힘들어짐. 이를 위해 이벤트가 발생하면 이를 로그에 기록하며 이는 대규모로 구축하기가 쉬웠음.
Kafka는 이러한 로그를 관리하는 시스템임. 카프카는 이벤트를 먼저 생각하고, 사물을 나중에 생각하도록 권장함.(Event)

기존의 모놀리스한 방식의 설계 방식은 더 이상의 확장이나 변경하기에 큰 어려움이 있음. 그러나 추세가 MSA(Micro Service Architecture)로 변해가며, 확장 및 변경에 용이한 구조가 되어가고 있음.
이러한 작은 각 서비스들은 Kafka를 통해 메시지를 소비하고 각 서버에서 그에 맞는 계산 혹은 로직을 수행하여, 다른 Kafka 주제로 해당 메시지를 생성하여 다른 서비스 및 기타 관심사에서 처리할 수 있도록 구성됨.
이렇게 모든 데이터가 **실시간 스트림**으로 흐르는 구조는 대규모 시스템에서의 **실시간**으로 분석하는 새로운 서비스를 구축하는 것들이 가능해짐.

또한 많은 시스템들이 얽혀있는 시스템구조에서 Kafka는 `connect`를 통해 각 외부 시스템간의 입력/출력을 담당할 수 있도록 제공하고 있음.
<img width="1826" height="1217" alt="image" src="https://github.com/user-attachments/assets/7fbe85e8-5b10-445a-b30e-f747d42f3748" />


### [Broker]

**Kafka Broker**는 일반적으로 `카프카`라고 불리는 시스템을 의미함. 카프카를 실행하면 카프카 서버 프로세스의 각 인스턴스(물리적 Server, Cloud instance 등)가 브로커가 됨.

이러한 Kafka Broker가 여러개 모인 그룹이 `Kafka Cluster`를 구성하게 됨.

---

#### 주요 역할

- **메시지 저장소**: Producer가 보낸 메시지를 디스크에 안전하게 저장
- **메시지 전달**: Consumer 요청에 따라 저장된 메시지를 전송
- **파티션 관리**: 토픽의 파티션들을 분산 저장하고 관리
- **복제 관리**: 데이터 손실 방지를 위해 다른 브로커에 데이터 복제

---

#### 핵심 특징

- **분산 처리**: 여러 브로커가 클러스터를 구성해 부하 분산
- **고성능**: 순차적 디스크 I/O로 높은 처리량 달성
- **내구성**: 메시지를 디스크에 영구 저장 (설정 가능한 보관 기간)
- **확장성**: 브로커 추가로 쉽게 용량과 성능 확장
- **장애 복구**: 브로커 장애 시 복제본을 통해 자동 복구
- **리더-팔로워 구조**: 각 파티션마다 하나의 리더 브로커가 읽기/쓰기 담당

---

#### Inside the Kafka Broker

<img width="2185" height="1121" alt="image" src="https://github.com/user-attachments/assets/62e74117-f8a9-47d3-b8e8-7b9ae95713ae" />

**Client 요청의 범주**
- 생성 요청
- 가져오기 요청

---

### 1. 생성 요청 (Produce Request)

#### 1) 파티션 할당

클라이언트가 카프카에 Produce하는 요청을 말한다. 이때 브로커는 구성 가능한 파티셔너를 사용하여 레코드에 할당할 토픽 파티션을 결정한다.

- **키가 존재하는 경우**: `키의 해시를 사용해 파티션을 결정`하며, 동일한 키를 가진 모든 레코드가 동일한 파티션에 할당됨
- **키가 없는 경우**: `파티션 전략`에 따라 데이터 균형을 맞춰 Produce함

#### 2) 레코드 배치

레코드를 한번에 하나씩 보낸다면 네트워크의 많은 오버헤드를 불러일으키며, 이를 위해 producer는 파티션에 할당된 레코드를 일괄 처리하여 누적함.

이러한 방식은 압축을 사용할 때 더 효과적인 압축을 제공함.

**배치 방식의 두 가지 기준**
- **시간 기준**: 이벤트 발행량이 많지 않을 경우 메시지들을 발행할 수 없는 상태가 되거나 매우 늦게 발행될 수 있음(실시간성 중요)
- **크기 기준**: 대량의 데이터를 전송하는 경우, 제한된 네트워크 환경에서 작업하는 경우, 실시간성이 크게 중요하지 않은 경우

#### 3) 네트워크 스레드가 요청을 큐에 추가

<img width="1777" height="831" alt="image" src="https://github.com/user-attachments/assets/199c02b8-5be8-4ca5-ad4e-33119591fa01" />

**처리 과정 (4단계)**
1. **요청 도착** → Socket receive buffer에 클라이언트 요청이 저장
2. **담당자 배정** → Network thread pool에서 하나의 스레드가 해당 요청을 담당
3. **데이터 변환** → Socket buffer에서 데이터를 읽어 produce request 객체로 변환
4. **대기열 등록** → 완성된 request 객체를 request queue에 추가

#### 4) I/O 스레드가 배치를 검증하고 저장

<img width="1765" height="957" alt="image" src="https://github.com/user-attachments/assets/26774ddd-7c16-45cb-b4d6-faff350b4c13" />

클라이언트의 요청이 대기열(큐)에 쌓이면, I/O 스레드 풀의 스레드가 큐에서 요청을 가져온다.

이때 **CRC(`Cyclic Redundancy Check` / 데이터 무결성 검사)** 검사를 포함한 몇가지 유효성 검사를 수행하며, 이를 커밋 로그라고 불리는 파티션의 물리적 데이터 구조에 추가한다.

##### 커밋 로그 (Commit Log)

**커밋 로그**는 메시지들이 **시간 순서대로** 쌓이는 `Append-only`의 로그 파일로 설정한 보관 기간(기본 7일)까지 디스크에 유지한다.

디스크 상에서의 **커밋 로그**는 `세그먼트` 모음으로 구성되며, 각 세그먼트는 여러 파일로 구성되어 있다:

- **`.log` 파일**: 이벤트 데이터가 포함
- **`.index` 파일**: 레코드 오프셋을 `.log` 파일 내 해당 레코드의 위치로 매핑하는 인덱스 구조

```text
Log Compaction & Tombstone

[Log Compaction]
- **사용 목적**: 같은 Key의 최신 값만 유지하고 싶을 때 (예: 사용자 프로필, 설정 정보 등)

압축 전:
key1: value1  (offset 0)
key2: value2  (offset 1)
key1: value3  (offset 2)  ← 최신값
key2: value4  (offset 3)  ← 최신값
압축 후:
key1: value3  (offset 2)  ← 최신값만 남김
key2: value4  (offset 3)  ← 최신값만 남김

[Tombstone]
key1: value1    (일반 메시지)
key1: null      ← Tombstone (삭제 마커)

**처리 과정**
1. 삭제하려면 → `key: null` 메시지 전송
2. Compaction 실행 시 → 해당 key의 모든 이전 메시지 삭제
3. 일정 시간 후 → Tombstone 자체도 삭제 (`delete.retention.ms`)

[전체 생명주기]
1. **일반 append** → `.log` 파일에 계속 추가
2. **Segment 가득참** → 새 segment 생성
3. **Compaction 트리거** → 중복 key 정리 + Tombstone 처리
4. **오래된 데이터** → retention 정책에 따라 삭제
```

#### 5) 클러스터 구조의 Kafka 연옥
로그 데이터가 페이지 캐시에서 디스크로 동기식으로 플러시되지 않기 때문에, Kafka는 내구성제공을 위해 여러 브로커 노드에 대한 복제에 의존한다. 기본적으로 브로커는 다른 브로커에 복제될 때까지 프로듀스 요청을 확인하지 않는다.
이러한 복제 단계를 기다리는 동안 I/O 스레드가 병목이 생기는 것을 방지하기 위해, 요청 객체는 `연옥(purgatory)`라고 불리는 맵 형태의 데이터 구조에 저장되어 대기한다. 이후 요청이 완전히 복제되면, 브로커는 요청 객체를 연옥에서 꺼내 응답객체를 생성한 후 
응답큐에 배치한다.
https://ferbncode.github.io/Apache-Kafka-and-Request-Purgatory.html

### Topic
토픽은 내구성 있는 방식으로 저장된 이벤트의 정렬된 컬렉션에 불과함. 이는 이벤트가 디스크에 기록되고 복제된다는 것을 의미함. 따라서 인프라가 운영되는 곳이라면
어디든 여러 개의 디스크, 여러 서버에 저장되기에 `하드웨어 오류로 인해 데이터가 사라지는 일이 없음`. 이러한 주제는 몇 시간, 몇 년, 무기한 등의 기간 동안 `데이터를 저장할 수 있음`.

### Partition
파티션은 Kafka에서 병렬 처리의 기본 단위임. 프로듀서와 브로커 모두 서로 다른 파티션에 대한 쓰기 작업을 병렬로 수행할 수 있으며, 이는 압축과 같은 비용이 많이 드는 작업에 하드웨어 자원을 확보해줌.
컨슈머의 경우, 파티션당 최대 하나의 컨슈머 인스턴스만 가동할 수  있으며, 그 이상은 `유휴` 상태가 됨.
```text
[파티션 공식]
최소 파티션 수를 결정하기 위한 공식

파티션 수 = max(t/p, t/c)

t = 목표 처리량
p = 단일 프로덕션 파티션에서 측정된 처리량
c = 소비량

[계산 과정]
t/p = 처리량 관점에서 필요한 파티션 수
t/c = 컨슈머 관점에서 필요한 파티션 수
max(t/p, t/c) = 두 값 중 더 큰 값을 선택

목표 처리량(t) = 1000 msg/sec
프로듀서 처리량(p) = 100 msg/sec/partition
컨슈머 처리량(c) = 150 msg/sec/partition

계산:
- t/p = 1000/100 = 10 파티션
- t/c = 1000/150 = 6.67 → 7 파티션 (올림)
- max(10, 7) = 10 파티션

producer가 달성할 수 있는 파티션당 처리량은 배치 크기, 압축, 목제 개수 등의 구성에 따라 달라질 수 있음.
consumer 처리량은 각 메시지를 처리하는 consumer의 로직 속도에 따라 달라짐.
```
#### 파티션과 메시지 키
파티션 수를 늘릴 수 있지만, 생성되는 메시지에 키가 포함된 경우 주의가 필요함. Kafka는 해당 메시지를 `키의 해시 기반`으로 파티션에 매핑시키기 때문임. 이는 동일한 키를 가진 메시지가 항상 같은 파티션으로 라우팅된다는 보장을 제공함.
파티션 수가 변경될 경우 이 전달 보장이 더 이상 유지되지 않을 수 있으며, 이런 상황을 피하기 위해선 **의도적으로 파티션을 과도하게 생성할 수 있음.** 즉, 현재 필요한 양보다 미래 성장에 기반한 설정 방식임.

#### 파티션 수 변경
기존 토픽의 파티션 수는 토픽별로 늘릴 수 있으며, 아래와 같은 명령을 통해 가능함.
```text
bin/kafka-topics.sh --bootstrap-server <broker:port> --topic <topic-name> --alter --partitions <number>
```
Zookeaper Kafka 기준 각 클러스터는 최대 20만개의 파티션을 가질 수 있으며, KRaft 환경에선 더 많은 파티션 수를 제공 가능함.
### Producer
Producer는  Kafka 클러스터에 이벤트를 게시하는 클라이언트 애플리케이션이다. 컨셉적으로 그룹 협조가 필요하지 않기 때문에 `Consumer`모다 훨씬 단순하다. Produce 파티셔너는 각 메시지를 토픽 파티션에 매핑하며, Producer는 해당 파티션의 리더에게
발행 요청을보낸다. 이때 Kafka와 함께 제공되는 파티셔너는 비어 있지 않은 동일한 키를 가진 모든 메시지가 동일한 파티션으로 전송되도록 보장한다.

**파티셔닝 방식**
키 있음: murmur2 해시 알고리즘으로 동일 키 → 동일 파티션 보장
키 없음: Sticky Partitioner 사용 (배치가 꽉 찰 때까지 같은 파티션, 새 배치는 랜덤)

**리더-팔로워 구조**
각 파티션마다 리더 1개 + 복제본(replica) 여러 개
모든 쓰기는 리더를 통해서만 가능
복제본들은 리더로부터 데이터를 복사해서 동기화

**메시지 가용성**
리더에 쓰기 완료 ≠ 즉시 읽기 가능
모든 in-sync replica가 복사 완료 → committed → 읽기 가능
이렇게 해야 브로커 장애 시에도 데이터 손실 방지

**내구성 vs 성능 트레이드오프**
acks=1 (리더만 확인): 빠르지만 리더 장애 시 데이터 손실 위험
acks=all (모든 replica 확인): 안전하지만 처리량 감소


#### Producer Config
**[필수 기본 설정]**
bootstrap.servers: 카프카 클러스터 주소 (필수!)
client.id: 클라이언트 식별자 (디버깅/모니터링용, 권장)

**[메시지 내구성 제어]**
acks - 얼마나 안전하게 보낼까?

acks=0: 최고 속도, 보장 없음 (fire and forget)
acks=1: 리더만 확인 (기본적인 안전성)
acks=all: 모든 복제본 확인 (최고 안전성, 기본값)

**[메시지 순서 보장]**
`retries + max.in.flight.requests.per.connection`

순서 유지 필요시: retries > 0 + max.in.flight.requests.per.connection=1
순서 상관없이 성능 우선: max.in.flight.requests.per.connection > 1

**[배치/압축으로 성능 최적화]**
배치 크기 조절:

batch.size: 배치 최대 크기 (Java)
batch.num.messages: 배치당 메시지 수 (C/C++, Python, Go, .NET)

**[배치 대기 시간]**
linger.ms: 배치가 찰 때까지 기다리는 시간 (↑값 = ↑처리량, ↑지연)

**[압축]**
compression.type: snappy, gzip, lz4 등

[메모리/큐 제한]
buffer.memory: 전체 메모리 한도 (Java)
request.timeout.ms: 요청 타임아웃 (무한 대기 방지)

```text
[설정 조합 예시]
# 고성능 설정
acks=1
linger.ms=5
batch.size=16384
compression.type=snappy

# 안전성 우선 설정  
acks=all
retries=10
max.in.flight.requests.per.connection=1
```
### Consumer
https://docs.confluent.io/platform/current/clients/consumer.html
### MetaData
이후에 더 깊게 다루겠지만 Kafka는 특정 버전(3.7.x) 이후로 `orchestration`의 주체가 **ZooKeeper** 에서 **KRAFT(Kafka Raft)**로 변경되었다.
이는 Zookeeper로 부터의 의존성을 떼어내기 위한 것으로 기존 메타데이터의 경우 ZooKeeper의 노드에서 관리했지만 KRAFT에선 이를 Kafka 클러스터 일부인 Kafka 컨트롤러 그룹 내에서 전파되도록 관리한다.

